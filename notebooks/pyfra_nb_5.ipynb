{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "607d5dc9",
   "metadata": {},
   "source": [
    "Notebook 5\n",
    "==============\n",
    "Further Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5441fde",
   "metadata": {},
   "source": [
    "# Outline\n",
    "The aim is to further investigate the models developed in the third notebook.\n",
    "We will\n",
    "1. Identify the relationship between amount of training data and model performance\n",
    "2. Compare the performance of our model with a naive approach of training on the un-stratified, imbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f43fe0",
   "metadata": {},
   "source": [
    "# Import Modules, Data and Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416edbd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import the modules\n",
    "import pyfra\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43509dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/df.p')\n",
    "n_rows_complete = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether or not the data is up-to-date (file can't be tracked on github because of it's file size)\n",
    "pd.testing.assert_frame_equal(left=(pd.read_csv('../data/df_check_info.csv', index_col=0)), \\\n",
    "                         right=pyfra.df_testing_info(df),\\\n",
    "                         check_dtype=False, check_exact=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640e4336",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f0458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(columns='Severity',axis=1).select_dtypes(include=np.number).dropna(axis=1)\n",
    "target = df['Severity']\n",
    "data, target = rus.fit_resample(X=data, y=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e325a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb1bf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We are working on {len(target)} data points, which represent {len(target)/n_rows_complete*100:.04f}% of the original data,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e861e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(columns='Severity',axis=1).select_dtypes(include=np.number).dropna(axis=1)\n",
    "target = df['Severity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c83b366",
   "metadata": {},
   "source": [
    "# Relation between Amount of Training Data and Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea888605",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = load('../models/preprocessing_pipeline.joblib')\n",
    "svc = load('../models/svc.joblib')\n",
    "stacking_clf = load('../models/stacking_clf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.verbose= 100\n",
    "stacking_clf.verbose = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99c5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a matrix to store the results\n",
    "result_metrics = pd.DataFrame(columns=['model', 'n_rows','f1', 'accuracy', 'recall'])\n",
    "result_metrics.index.name = 'id'\n",
    "result_metrics\n",
    "result_metrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd960582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample of the data, because the whole dataset is too big for us to work with\n",
    "#df = df.sample(n=n_rows, random_state=23)\n",
    "from sklearn.utils import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f8a580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to compute and store the results for the respective model\n",
    "from sklearn.utils import random\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score\n",
    "def store_metrics(model_label, model, n_rows, result_df):\n",
    "    id = result_df.shape[0]\n",
    "    result_df.loc[id, 'model_label'] = model_label\n",
    "    result_df.loc[id, 'model'] = model\n",
    "    result_df.loc[id, 'n_rows'] = n_rows\n",
    "    print(f'Splitting {n_rows} rows of data...')\n",
    "    sample_indices = random.sample_without_replacement(n_population=len(target), \n",
    "                                                       n_samples=n_rows)\n",
    "    data_sample = data.iloc[sample_indices]\n",
    "    target_sample = target.iloc[sample_indices]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_sample, \n",
    "                                                        target_sample, \n",
    "                                                        test_size=0.2, \n",
    "                                                        random_state=23, \n",
    "                                                        stratify=target_sample)\n",
    "    print(f'Preprocessing Data...')\n",
    "    X_train = preprocessing_pipeline.fit_transform(X_train, y_train)\n",
    "    X_test = preprocessing_pipeline.transform(X_test)\n",
    "    print(f'Fitting {model_label}...')\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f'Predicting...')\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'Computing scores...')\n",
    "    result_df.loc[id, 'f1'] = f1_score(y_true=y_test, y_pred=y_pred, average='weighted')\n",
    "    result_df.loc[id, 'accuracy'] = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "    result_df.loc[id, 'recall'] = recall_score(y_true=y_test, y_pred=y_pred, average='weighted')\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f054a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for n_rows in [500, 1_000, 2_000, 5_000, 10_000, 20_000]:\n",
    "    result_metrics = store_metrics('stacking', stacking_clf, n_rows, result_metrics)\n",
    "print(result_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a80833",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d5fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result_metrics['n_rows'],result_metrics['f1'],'x-', label='$f_1$');\n",
    "#plt.plot(result_metrics['n_rows'],result_metrics['accuracy'], label='Accuracy');\n",
    "plt.plot(result_metrics['n_rows'],result_metrics['recall'],'o-', label='Recall');\n",
    "plt.title('Performance in Relation to the Amount of Input Data')\n",
    "plt.xlabel('Number of Data Points')\n",
    "plt.ylabel('Performance')\n",
    "plt.ylim((0,0.5))\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-LanguageId",
   "main_language": "python",
   "notebook_metadata_filter": "-kernelspec"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
