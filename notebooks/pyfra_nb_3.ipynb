{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a798b2",
   "metadata": {},
   "source": [
    "Notebook 3\n",
    "==============\n",
    "Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575d177",
   "metadata": {},
   "source": [
    "# Importing Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d0d84",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pyfra\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier \n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, make_scorer\n",
    "from imblearn import under_sampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from time import sleep\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/df.p')\n",
    "n_rows_complete = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e858a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether or not the data is up-to-date (file can't be tracked on github because of it's file size)\n",
    "pd.testing.assert_frame_equal(left=(pd.read_csv('../data/df_check_info.csv', index_col=0)), \\\n",
    "                         right=pyfra.df_testing_info(df),\\\n",
    "                         check_dtype=False, check_exact=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46701a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a95d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample of the data, because the whole dataset is too big for us to work with\n",
    "relative_sample_size = 0.01\n",
    "df = df.sample(frac=relative_sample_size, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbad5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(columns='Severity',axis=1).select_dtypes(include=np.number).dropna(axis=1)\n",
    "target = df['Severity']\n",
    "data, target = rus.fit_resample(X=data, y=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a061d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We are working on {len(target)} data points, which represent {len(target)/n_rows_complete*100:.04f}% of the original data,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(data, target, test_size=0.2 ,random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2b2dd",
   "metadata": {},
   "source": [
    "# Scaling the Data and Selecting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = std_scaler.transform(X_train)\n",
    "X_test_scaled = std_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_features = 25\n",
    "kbest_selector = SelectKBest(k=k_features)\n",
    "kbest_selector.fit(X_train_scaled,y_train);\n",
    "X_train_scaled_selection = kbest_selector.transform(X_train_scaled)\n",
    "X_test_scaled_selection = kbest_selector.transform(X_test_scaled)\n",
    "print(f'We use {k_features} of the original {df.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba286e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_best_feature_names = data.columns[kbest_selector.get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b8af64",
   "metadata": {},
   "source": [
    "# Application of Machine Learning Models\n",
    "## Setup of Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3424ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a matrix to store the results\n",
    "result_metrics = pd.DataFrame(columns=['model', 'f1', 'accuracy', 'recall'])\n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871080f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to compute and store the results for the respective model\n",
    "def store_metrics(model_name, model, y_test, y_pred, result_df):\n",
    "    result_df.loc[model_name, 'model'] = model\n",
    "    result_df.loc[model_name, 'f1'] = f1_score(y_true=y_test, y_pred=y_pred, average='weighted')\n",
    "    result_df.loc[model_name, 'accuracy'] = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "    result_df.loc[model_name, 'recall'] = recall_score(y_true=y_test, y_pred=y_pred, average='weighted')\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473cf26f",
   "metadata": {},
   "source": [
    "## Setup of the Cross-Validator\n",
    "We will use a repeated stratified cross-validation to make sure to pick the best parameters.\n",
    "The stratification will be used to ensure an equal distribution of the different categories in every bin.\n",
    "The repetition will be used in order ensure that the result is not an outlier. We will set a lower the number of repetitions, however, to save execution time (default would be 10 repetitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f21e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b813ae",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "\n",
    "A support vector machine classifier will be used with parameter optimization via grid search.\n",
    "\n",
    "### Setup of the SVM and the Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd4ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of the SVM Classifier\n",
    "# We set the cache size to 1600 MB (default: 200 MB) to reduce the computing time.\n",
    "# The other parameters will be set via grid search.\n",
    "svc = svm.SVC(cache_size=1600)\n",
    "\n",
    "# Choosing the parameters for the grid search\n",
    "svc_params = {\n",
    "    'kernel': ['poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': [0.1, 0.5, 'scale'],\n",
    "    'C': [0.1, 0.5, 1, 2]\n",
    "}\n",
    "\n",
    "# Setup of the scoring. \n",
    "# We have to define the parameter 'average', because we are not dealing with a binary classification.\n",
    "# Our sample is balanced, hence we can use a simple approach, using 'micro', which uses the global values of \n",
    "# true positives, false negatives and false positives.\n",
    "f1_scoring = make_scorer(score_func=f1_score, average='micro')\n",
    "\n",
    "# Instantiation of the GridSearchCv\n",
    "# n_jobs is set to -1 to use all available threads for computation.\n",
    "svc_grid = GridSearchCV(svc, param_grid=svc_params, scoring=f1_scoring, cv=cv, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf55ed3a",
   "metadata": {},
   "source": [
    "### SVM Parameter Optimization, Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the grid search to find the best parameter combination\n",
    "svc_grid.fit(X_train_scaled_selection, y_train)\n",
    "\n",
    "# Print result of parameter optimization\n",
    "print('Best parameter combination: ',svc_grid.best_params_)\n",
    "\n",
    "# Predict target variable for the test set\n",
    "svc = svc_grid.best_estimator_\n",
    "y_svc = svc.predict(X_test_scaled_selection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255a8b88",
   "metadata": {},
   "source": [
    "### Metrics of SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7caa28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the metrics for the optimal svm model and store them in the result_metrics DataFrame \n",
    "# The model will be stored as well in the DataFrame\n",
    "result_metrics = store_metrics(model=svc, model_name='Support Vector Machine',\n",
    "                               y_test=y_test, y_pred=y_svc,\n",
    "                               result_df=result_metrics)\n",
    "# Show the interim result                               \n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb63406c",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "### Setup and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49181ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': [5,10,20],\n",
    "    'min_samples_leaf':[3,7,15],\n",
    "    'n_estimators': [50,100]\n",
    "    }\n",
    "\n",
    "RFCLF = GridSearchCV(RandomForestClassifier(),param_grid = params, cv = cv)\n",
    "RFCLF.fit(X_train_scaled_selection,y_train)\n",
    "\n",
    "print('Best Params are:',RFCLF.best_params_)\n",
    "print('Best Score is:',RFCLF.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa49bf0",
   "metadata": {},
   "source": [
    "### Optimized Model and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664af609",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RFCLF.best_estimator_\n",
    "y_rf = rf.predict(X_test_scaled_selection)\n",
    "\n",
    "cm = pd.crosstab(y_test,y_rf, rownames=['Real'], colnames=['Prediction'])\n",
    "print(cm)\n",
    "\n",
    "result_metrics = store_metrics(model=rf, model_name='Random Forest',\n",
    "                               y_test=y_test, y_pred=y_rf,\n",
    "                               result_df=result_metrics)\n",
    "                              \n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ffc7b5",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use and define logistic Regression with n_jobs=-1 to use all cores\n",
    "LR = LogisticRegression()\n",
    "#for parameters we use 3 type of solver and 6 for C\n",
    "LR_params = {\n",
    "    'solver': ['liblinear', 'lbfgs', 'saga'], \n",
    "    'C': [10**(i) for i in range(-5, 5)]\n",
    "}\n",
    "\n",
    "f1_scoring = make_scorer(score_func=f1_score, average='micro')\n",
    "\n",
    "# Instantiation of the GridSearchCv\n",
    "LR_grid = GridSearchCV(LR, param_grid=LR_params, scoring=f1_scoring, cv=cv, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5677f865",
   "metadata": {},
   "source": [
    "# LR Parameter Optimization, Training and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc95782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the grid search to find the best parameter combination\n",
    "LR_grid.fit(X_train_scaled_selection, y_train)\n",
    "\n",
    "# Print result of parameter optimization\n",
    "print('Best parameter combination: ',LR_grid.best_params_)\n",
    "\n",
    "# Predict target variable for the test set\n",
    "LR = LR_grid.best_estimator_\n",
    "y_LR = LR.predict(X_test_scaled_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7a53e4",
   "metadata": {},
   "source": [
    "## Metrics of LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee1bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the metrics for the optimal LR model and store them in the result_metrics DataFrame \n",
    "# The model will be stored as well in the DataFrame\n",
    "result_metrics = store_metrics(model=LR, model_name='Logistic Regression',\n",
    "                               y_test=y_test, y_pred=y_LR,\n",
    "                               result_df=result_metrics)\n",
    "# Show the interim result                               \n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85204aa6",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803571d",
   "metadata": {},
   "source": [
    "## Setup of the DT and the Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Grid\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [2,4,6,8,10,12]\n",
    "parameters = dict(criterion=criterion, max_depth=max_depth)\n",
    "\n",
    "DT = GridSearchCV(DecisionTreeClassifier(),param_grid = parameters, cv = RepeatedKFold(n_splits=4, n_repeats=1, random_state=23))\n",
    "# \n",
    "DT.fit(X_train_scaled_selection,y_train)\n",
    "\n",
    "# \n",
    "print('Best Criterion:', DT.best_estimator_.get_params())\n",
    "print('Best max_depth:', DT.best_estimator_.get_params())\n",
    "print(); print(DT.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1926b272",
   "metadata": {},
   "source": [
    "## Metrics of Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DT.best_estimator_\n",
    "y_dt = dt.predict(X_test_scaled_selection)\n",
    "cm = pd.crosstab(y_test,y_dt, rownames=['Real'], colnames=['Prediction'])\n",
    "print(cm)\n",
    "result_metrics = store_metrics(model=dt, model_name='Decision Tree',\n",
    "                               y_test=y_test, y_pred=y_dt,\n",
    "                               result_df=result_metrics)\n",
    "                              \n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9195ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Interpretation of the Decision Tree\n",
    "# Decision trees are known to have a high interpretability compared to other machine learning models. The performance of the applied model is worse than the ones of the other models, but we can easily plot the tree and gain insights.\n",
    "from sklearn.tree import plot_tree\n",
    "fig = plt.figure(figsize=(12,6));\n",
    "plot_tree(dt,max_depth=2, fontsize=8, feature_names=k_best_feature_names);\n",
    "\n",
    "# The plot shows that the most important feature (according to the decision tree) is built-up_area. This binary variable cointains the information, whether the accident happened in a built-up area. We already showed in the first notebook that there seems to be a positive relation between the density of an area and it's **number** of accident. The decision tree here suggests that the **severity** is also affected by a dense population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1da022",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Application of Advanced Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d7e3d6",
   "metadata": {},
   "source": [
    "## Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da2ade",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "estimators = [('lr', LR), ('svc', svc), ('rf', rf)]\n",
    "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=svc, cv='prefit', n_jobs=-1)\n",
    "\n",
    "stacking_clf.fit(X_train_scaled_selection, y_train)\n",
    "y_stacking = stacking_clf.predict(X_test_scaled_selection)\n",
    "result_metrics = store_metrics(model=stacking_clf, model_name='Stacking',\n",
    "                               y_test=y_test, y_pred=y_stacking,\n",
    "                               result_df=result_metrics)\n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c0444",
   "metadata": {},
   "source": [
    "# ADA Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d078ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying ADA boosting on LogisticRegresiion\n",
    "ADA_Boost = AdaBoostClassifier(estimator = LR , n_estimators = 1000)\n",
    "ADA_Boost.fit(X_train_scaled_selection, y_train)\n",
    "y_ada = ADA_Boost.predict(X_test_scaled_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1167230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_metrics = store_metrics(model=ADA_Boost, model_name='ADA Boost',\n",
    "                               y_test=y_test, y_pred=y_ada,\n",
    "                               result_df=result_metrics)\n",
    "# Show the interim result                               \n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0a686",
   "metadata": {},
   "source": [
    "\n",
    "# Results and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4ae8c",
   "metadata": {},
   "source": [
    "## Comparison of the Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f41a2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(y=result_metrics.index.values, width=result_metrics['f1']);\n",
    "plt.title('$F_1$ Score of different ML models');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12929fe1",
   "metadata": {},
   "source": [
    "The results show a comparable performance for all machine learning models, with the advanced stacking classifier giving the best score (f_1) and the decision tree giving the worst score. \n",
    "\n",
    "There are a few things to consider when analyzing these results.\n",
    "1. We worked on a very small partition of the dataset in order to achieve acceptable execution times. We expect to reach higher performances when working with more data.\n",
    "2. The classifiers based on logistic regression and decision trees have low scores, are not necessarily unfit for the dataset, as they offer more interpretability than the advanced models. This interpretability could help e.g. policy makers to take measures in order to reduce the severity of road accidents.\n",
    "3. A strong correlation between severity and safety measurements (e.g. safety belt) is expected. Unfortunately, this feature could not be used because useful is only available for the last years (2018--)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1708807",
   "metadata": {},
   "source": [
    "## Analysis of the Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37607de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.crosstab(y_test, y_stacking, rownames=['observations'], colnames=['predictions']);\n",
    "severity_categories = (\"Unscathed\",\"Killed\", \"Hospitalized\\nwounded\", \"Light injury\")\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.title('Correlation Matrix of the Stacking Classifier');\n",
    "sns.heatmap(cm, annot=True);\n",
    "plt.xticks(np.array(range(4))+0.5, labels=severity_categories, rotation=45);\n",
    "plt.yticks(np.array(range(4))+0.5, labels=severity_categories, rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731821cc",
   "metadata": {},
   "source": [
    "The correlation matrix of the stacking classifier shows that some categories are more difficult to predict than others. The category \"Hospitalized wounded\" seems to be the most difficult to predict, as the predictions seem to be quite evenly distributed between the different classes. We can quantify these difficulties by looking at the scores for accuracy and recall for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6164f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "severity_categories = (\"Unscathed\",\"Killed\", \"Hospitalized wounded\", \"Light injury\")\n",
    "print(classification_report(y_true=y_test, y_pred=y_stacking, target_names=severity_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b691385",
   "metadata": {},
   "source": [
    "The classification report reflects our observations from the correlation matrix. It is satisfying that the categorie \"Killed\" is predicted with the highest accuracy; we consider this category as particularly important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8435595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the models for further use and investigation\n",
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "dump(stacking_clf, '../models/stacking_clf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b2ea1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "loaded_model = load('../models/stacking_clf.joblib')\n",
    "y_test_loaded = loaded_model.predict(X_test_scaled_selection)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-LanguageId",
   "formats": "ipynb,py:light",
   "main_language": "python",
   "notebook_metadata_filter": "-kernelspec"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
